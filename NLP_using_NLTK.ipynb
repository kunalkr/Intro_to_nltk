{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Segmentation\n",
    "Text segmentation is the process of transforming text into meaningful units from text data. These units can be words, sentences or different topics (collection of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kkumar/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"CODE is founded by Mr. Bachem. Studying at CODE will be unlike any other higher education experience. Our intensive, interdisciplinary bachelor’s programs are designed to dramatically improve the way you work and to prepare you for the reality of tomorrow’s workplace.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CODE is founded by Mr. Bachem.', 'Studying at CODE will be unlike any other higher education experience.', 'Our intensive, interdisciplinary bachelor’s programs are designed to dramatically improve the way you work and to prepare you for the reality of tomorrow’s workplace.']\n"
     ]
    }
   ],
   "source": [
    "# get the text split into sentences\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CODE', 'is', 'founded', 'by', 'Mr.', 'Bachem', '.', 'Studying', 'at', 'CODE', 'will', 'be', 'unlike', 'any', 'other', 'higher', 'education', 'experience', '.', 'Our', 'intensive', ',', 'interdisciplinary', 'bachelor', '’', 's', 'programs', 'are', 'designed', 'to', 'dramatically', 'improve', 'the', 'way', 'you', 'work', 'and', 'to', 'prepare', 'you', 'for', 'the', 'reality', 'of', 'tomorrow', '’', 's', 'workplace', '.']\n"
     ]
    }
   ],
   "source": [
    "# get the text split into words\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words and Word Segmentation\n",
    "Also part of Natural Language are words (common words that do not contribute significantly to our cause) that are basically useless, which are referred to as \"stop words\". Since these words extend out processing time or take up unnecessary space in our database, we will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kkumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import stopwords from nltk package\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the text of stopwords\n",
    "filtered_tokens = [w for w in tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CODE', 'is', 'founded', 'by', 'Mr.', 'Bachem', '.', 'Studying', 'at', 'CODE', 'will', 'be', 'unlike', 'any', 'other', 'higher', 'education', 'experience', '.', 'Our', 'intensive', ',', 'interdisciplinary', 'bachelor', '’', 's', 'programs', 'are', 'designed', 'to', 'dramatically', 'improve', 'the', 'way', 'you', 'work', 'and', 'to', 'prepare', 'you', 'for', 'the', 'reality', 'of', 'tomorrow', '’', 's', 'workplace', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CODE', 'founded', 'Mr.', 'Bachem', '.', 'Studying', 'CODE', 'unlike', 'higher', 'education', 'experience', '.', 'Our', 'intensive', ',', 'interdisciplinary', 'bachelor', '’', 'programs', 'designed', 'dramatically', 'improve', 'way', 'work', 'prepare', 'reality', 'tomorrow', '’', 'workplace', '.']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming and Lemmatization are the process of getting the root/base form of a word by removing the derivational terms. In stemming we basically chose a crude way to achieve this by simply chopping up the derivational term in hope of getting the base form of any word. While in lemmatization we go for finding the base form matching a dictionary word, hence giving us more meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = [\"ride\", \"riding\", \"rider\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride\n",
      "ride\n",
      "rider\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"\"\"CODE is a newly founded private university of applied sciences that is embedded into the vibrant \n",
    "network of Berlin's digital economy.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code\n",
      "is\n",
      "a\n",
      "newli\n",
      "found\n",
      "privat\n",
      "univers\n",
      "of\n",
      "appli\n",
      "scienc\n",
      "that\n",
      "is\n",
      "embed\n",
      "into\n",
      "the\n",
      "vibrant\n",
      "network\n",
      "of\n",
      "berlin\n",
      "'s\n",
      "digit\n",
      "economi\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing (Speech tagging & Chunking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Speech Tagging\n",
    "Speech tagging in NLP is the process of labelling words in a sentence as specifice part of speech type (nouns, adjectives etc).\n",
    "\n",
    "NLTK provides us with a sentence tokenizer called the \"PunktSentenceTokenizer\", which is an unsupervised ML algorithm implementation that can be trained on any text corpus you wish to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/kkumar/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/kkumar/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# load novels from chesterton\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "test = gutenberg.raw(\"chesterton-ball.txt\")\n",
    "train = gutenberg.raw(\"chesterton-brown.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tokenizer\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train)\n",
    "\n",
    "# Tokenize the data\n",
    "tokenized_data = custom_sent_tokenizer.tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'IN'), ('The', 'DT'), ('Ball', 'NNP'), ('and', 'CC'), ('The', 'DT'), ('Cross', 'NNP'), ('by', 'IN'), ('G.K', 'NNP'), ('.', '.')]\n",
      "[('Chesterton', 'NNP'), ('1909', 'CD'), (']', 'NN'), ('I', 'PRP'), ('.', '.')]\n",
      "[('A', 'DT'), ('DISCUSSION', 'NNP'), ('SOMEWHAT', 'NNP'), ('IN', 'NNP'), ('THE', 'NNP'), ('AIR', 'NNP'), ('The', 'DT'), ('flying', 'VBG'), ('ship', 'NN'), ('of', 'IN'), ('Professor', 'NNP'), ('Lucifer', 'NNP'), ('sang', 'VBD'), ('through', 'IN'), ('the', 'DT'), ('skies', 'NNS'), ('like', 'IN'), ('a', 'DT'), ('silver', 'NN'), ('arrow', 'NN'), (';', ':'), ('the', 'DT'), ('bleak', 'JJ'), ('white', 'JJ'), ('steel', 'NN'), ('of', 'IN'), ('it', 'PRP'), (',', ','), ('gleaming', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('bleak', 'JJ'), ('blue', 'JJ'), ('emptiness', 'NN'), ('of', 'IN'), ('the', 'DT'), ('evening', 'NN'), ('.', '.')]\n",
      "[('That', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('far', 'RB'), ('above', 'IN'), ('the', 'DT'), ('earth', 'NN'), ('was', 'VBD'), ('no', 'DT'), ('expression', 'NN'), ('for', 'IN'), ('it', 'PRP'), (';', ':'), ('to', 'TO'), ('the', 'DT'), ('two', 'CD'), ('men', 'NNS'), ('in', 'IN'), ('it', 'PRP'), (',', ','), ('it', 'PRP'), ('seemed', 'VBD'), ('to', 'TO'), ('be', 'VB'), ('far', 'RB'), ('above', 'IN'), ('the', 'DT'), ('stars', 'NNS'), ('.', '.')]\n",
      "[('The', 'DT'), ('professor', 'NN'), ('had', 'VBD'), ('himself', 'PRP'), ('invented', 'VBN'), ('the', 'DT'), ('flying', 'VBG'), ('machine', 'NN'), (',', ','), ('and', 'CC'), ('had', 'VBD'), ('also', 'RB'), ('invented', 'VBN'), ('nearly', 'RB'), ('everything', 'NN'), ('in', 'IN'), ('it', 'PRP'), ('.', '.')]\n",
      "[('Every', 'DT'), ('sort', 'NN'), ('of', 'IN'), ('tool', 'NN'), ('or', 'CC'), ('apparatus', 'NN'), ('had', 'VBD'), (',', ','), ('in', 'IN'), ('consequence', 'NN'), (',', ','), ('to', 'TO'), ('the', 'DT'), ('full', 'JJ'), (',', ','), ('that', 'IN'), ('fantastic', 'JJ'), ('and', 'CC'), ('distorted', 'JJ'), ('look', 'NN'), ('which', 'WDT'), ('belongs', 'VBZ'), ('to', 'TO'), ('the', 'DT'), ('miracles', 'NNS'), ('of', 'IN'), ('science', 'NN'), ('.', '.')]\n",
      "[('For', 'IN'), ('the', 'DT'), ('world', 'NN'), ('of', 'IN'), ('science', 'NN'), ('and', 'CC'), ('evolution', 'NN'), ('is', 'VBZ'), ('far', 'RB'), ('more', 'JJR'), ('nameless', 'JJ'), ('and', 'CC'), ('elusive', 'JJ'), ('and', 'CC'), ('like', 'IN'), ('a', 'DT'), ('dream', 'NN'), ('than', 'IN'), ('the', 'DT'), ('world', 'NN'), ('of', 'IN'), ('poetry', 'NN'), ('and', 'CC'), ('religion', 'NN'), (';', ':'), ('since', 'IN'), ('in', 'IN'), ('the', 'DT'), ('latter', 'JJ'), ('images', 'NNS'), ('and', 'CC'), ('ideas', 'NNS'), ('remain', 'VBP'), ('themselves', 'PRP'), ('eternally', 'RB'), (',', ','), ('while', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('whole', 'JJ'), ('idea', 'NN'), ('of', 'IN'), ('evolution', 'NN'), ('that', 'IN'), ('identities', 'VBZ'), ('melt', 'FW'), ('into', 'IN'), ('each', 'DT'), ('other', 'JJ'), ('as', 'IN'), ('they', 'PRP'), ('do', 'VBP'), ('in', 'IN'), ('a', 'DT'), ('nightmare', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "def tag_text(data):\n",
    "    try:\n",
    "        for i in data[:7]:\n",
    "            actual_words = nltk.word_tokenize(i)\n",
    "            tagged_words = nltk.pos_tag(actual_words)\n",
    "            print(tagged_words)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "tag_text(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Chunking Text\n",
    "\n",
    "Chunking is the process of grouping words into more meaningful chunks than mere speech tags. For example noun and verb phrases. With the help of chunking you can build a parsen tree.\n",
    "\n",
    "In this example we will search for chunks that corresponds to invidual noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pre-tagged text for simplicity\n",
    "text = [(\"the\", \"DT\"), (\"huge\", \"JJ\"), (\"german\", \"JJ\"), (\"Rottweiler\", \"NN\"), \n",
    "        (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT huge/JJ german/JJ Rottweiler/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "# define a noun phrase as:\n",
    "#     NP = determiner + adjective + singular_noun\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# apply grammar to regexparser\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "# carry out chunking\n",
    "result = cp.parse(text)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
